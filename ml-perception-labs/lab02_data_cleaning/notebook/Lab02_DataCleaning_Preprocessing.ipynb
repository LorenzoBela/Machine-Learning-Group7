{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230eab5b",
   "metadata": {},
   "source": [
    "# Laboratory Exercise 2: Data Cleaning and Preprocessing\n",
    "\n",
    "**Name:** Lorenzo Bela, Robert Callorina, Kean Guzon\n",
    "\n",
    "**Section:** 58036\n",
    "\n",
    "**Date:** January 26 2026\n",
    "\n",
    "**Dataset:** CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8228c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b056811",
   "metadata": {},
   "source": [
    "## Part B: Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = Path('../../lab01_data_value_chain/data/raw/cifar-10-batches-py')\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "train_batches = []\n",
    "for i in range(1, 6):\n",
    "    batch_file = raw_data_path / f'data_batch_{i}'\n",
    "    batch_data = unpickle(batch_file)\n",
    "    train_batches.append(batch_data)\n",
    "\n",
    "test_batch = unpickle(raw_data_path / 'test_batch')\n",
    "meta = unpickle(raw_data_path / 'batches.meta')\n",
    "\n",
    "train_images = np.concatenate([batch[b'data'] for batch in train_batches])\n",
    "train_labels = np.concatenate([batch[b'labels'] for batch in train_batches])\n",
    "test_images = test_batch[b'data']\n",
    "test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "train_images = train_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "test_images = test_images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "all_images = np.concatenate([train_images, test_images])\n",
    "all_labels = np.concatenate([train_labels, test_labels])\n",
    "\n",
    "print(f\"Total samples: {len(all_images)}\")\n",
    "print(f\"Image shape: {all_images[0].shape}\")\n",
    "print(f\"Data type: {all_images.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [name.decode('utf-8') for name in meta[b'label_names']]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "random_indices = np.random.choice(len(all_images), 10, replace=False)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img_idx = random_indices[idx]\n",
    "    ax.imshow(all_images[img_idx])\n",
    "    ax.set_title(label_names[all_labels[img_idx]])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/lab02_raw_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d45f",
   "metadata": {},
   "source": [
    "## Part C: Detect Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = []\n",
    "valid_samples = 0\n",
    "invalid_samples = 0\n",
    "\n",
    "for i in range(len(all_images)):\n",
    "    img = all_images[i]\n",
    "    label = all_labels[i]\n",
    "    \n",
    "    if img.shape != (32, 32, 3):\n",
    "        issues.append(f\"Sample {i}: Invalid shape {img.shape}\")\n",
    "        invalid_samples += 1\n",
    "        continue\n",
    "    \n",
    "    if label < 0 or label >= 10:\n",
    "        issues.append(f\"Sample {i}: Invalid label {label}\")\n",
    "        invalid_samples += 1\n",
    "        continue\n",
    "    \n",
    "    if np.isnan(img).any() or np.isinf(img).any():\n",
    "        issues.append(f\"Sample {i}: Contains NaN or Inf values\")\n",
    "        invalid_samples += 1\n",
    "        continue\n",
    "    \n",
    "    valid_samples += 1\n",
    "\n",
    "print(f\"Valid samples: {valid_samples}\")\n",
    "print(f\"Invalid samples: {invalid_samples}\")\n",
    "\n",
    "log_path = Path('../outputs/logs')\n",
    "log_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(log_path / 'lab02_data_issues.txt', 'w') as f:\n",
    "    f.write(f\"Data Quality Report\\n\")\n",
    "    f.write(f\"Total samples: {len(all_images)}\\n\")\n",
    "    f.write(f\"Valid samples: {valid_samples}\\n\")\n",
    "    f.write(f\"Invalid samples: {invalid_samples}\\n\\n\")\n",
    "    f.write(\"Issues detected:\\n\")\n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            f.write(f\"  {issue}\\n\")\n",
    "    else:\n",
    "        f.write(\"  No issues detected\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adc647",
   "metadata": {},
   "source": [
    "## Part D: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8916f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = []\n",
    "\n",
    "for i in range(len(all_images)):\n",
    "    img = all_images[i]\n",
    "    label = all_labels[i]\n",
    "    \n",
    "    if img.shape == (32, 32, 3) and 0 <= label < 10 and not (np.isnan(img).any() or np.isinf(img).any()):\n",
    "        valid_indices.append(i)\n",
    "\n",
    "clean_images = all_images[valid_indices]\n",
    "clean_labels = all_labels[valid_indices]\n",
    "\n",
    "print(f\"Original dataset size: {len(all_images)}\")\n",
    "print(f\"Cleaned dataset size: {len(clean_images)}\")\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['Original Size', 'Cleaned Size', 'Removed Samples'],\n",
    "    'Count': [len(all_images), len(clean_images), len(all_images) - len(clean_images)]\n",
    "})\n",
    "\n",
    "summary_df.to_csv('../outputs/tables/lab02_cleaning_summary.csv', index=False)\n",
    "print(\"\\nCleaning Summary:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0b98b",
   "metadata": {},
   "source": [
    "## Part E: Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4800c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images, target_size):\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        pil_img = Image.fromarray(img.astype('uint8'))\n",
    "        resized = pil_img.resize((target_size, target_size), Image.BILINEAR)\n",
    "        arr = np.array(resized, dtype=np.float32)\n",
    "        normalized = arr / 255.0\n",
    "        processed.append(normalized)\n",
    "    return np.array(processed)\n",
    "\n",
    "processed_64 = preprocess_images(clean_images, 64)\n",
    "processed_128 = preprocess_images(clean_images, 128)\n",
    "\n",
    "print(f\"64x64 - Final shape: {processed_64[0].shape}\")\n",
    "print(f\"64x64 - Value range: [{processed_64.min():.4f}, {processed_64.max():.4f}]\")\n",
    "print(f\"128x128 - Final shape: {processed_128[0].shape}\")\n",
    "print(f\"128x128 - Value range: [{processed_128.min():.4f}, {processed_128.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1595230",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "sample_indices = np.random.choice(len(clean_images), 5, replace=False)\n",
    "\n",
    "for idx in range(5):\n",
    "    img_idx = sample_indices[idx]\n",
    "    \n",
    "    axes[0, idx].imshow(clean_images[img_idx].astype('uint8'))\n",
    "    axes[0, idx].set_title(f\"Raw (32x32)\")\n",
    "    axes[0, idx].axis('off')\n",
    "    \n",
    "    axes[1, idx].imshow(processed_64[img_idx])\n",
    "    axes[1, idx].set_title(f\"Processed (64x64)\")\n",
    "    axes[1, idx].axis('off')\n",
    "    \n",
    "    axes[2, idx].imshow(processed_128[img_idx])\n",
    "    axes[2, idx].set_title(f\"Processed (128x128)\")\n",
    "    axes[2, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/lab02_processed_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee464a",
   "metadata": {},
   "source": [
    "## Part F: Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path('../data/processed')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(processed_path / 'images_64x64.npy', processed_64)\n",
    "np.save(processed_path / 'images_128x128.npy', processed_128)\n",
    "np.save(processed_path / 'labels.npy', clean_labels)\n",
    "\n",
    "print(\"Saved processed datasets:\")\n",
    "print(f\"  - images_64x64.npy: {processed_64.shape}\")\n",
    "print(f\"  - images_128x128.npy: {processed_128.shape}\")\n",
    "print(f\"  - labels.npy: {clean_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "metadata = f\"\"\"Preprocessing Metadata\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Dataset: CIFAR-10\n",
    "Original shape: (32, 32, 3)\n",
    "Processed shapes: (64, 64, 3) and (128, 128, 3)\n",
    "Normalization method: Min-Max scaling to [0, 1]\n",
    "Data type: float32\n",
    "Total samples: {len(clean_labels)}\n",
    "Resizing method: Bilinear interpolation\n",
    "\"\"\"\n",
    "\n",
    "with open('../outputs/logs/lab02_preprocessing_metadata.txt', 'w') as f:\n",
    "    f.write(metadata)\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf4103",
   "metadata": {},
   "source": [
    "## Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b0189",
   "metadata": {},
   "source": [
    "### A. Raw vs Processed Data Comparison\n",
    "\n",
    "**Original:**\n",
    "- Shape: (32, 32, 3)\n",
    "- Data type: uint8\n",
    "- Value range: [0, 255]\n",
    "\n",
    "**Processed (64x64):**\n",
    "- Shape: (64, 64, 3)\n",
    "- Data type: float32\n",
    "- Value range: [0.0, 1.0]\n",
    "\n",
    "**Processed (128x128):**\n",
    "- Shape: (128, 128, 3)\n",
    "- Data type: float32\n",
    "- Value range: [0.0, 1.0]\n",
    "\n",
    "Images were resized to standard dimensions and normalized to improve training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791a68b",
   "metadata": {},
   "source": [
    "### B. Data Quality Findings\n",
    "\n",
    "The CIFAR-10 dataset is clean with no corrupted samples detected. All images have consistent dimensions and valid labels. This demonstrates the importance of using well-maintained benchmark datasets, though real-world data often requires more extensive cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039b1f9",
   "metadata": {},
   "source": [
    "### C. Visual Comparison\n",
    "\n",
    "The processed images maintain visual quality while standardizing dimensions. Normalization does not alter appearance but ensures pixel values are in an optimal range for neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb8bd3",
   "metadata": {},
   "source": [
    "### D. Impact on Model Readiness\n",
    "\n",
    "Preprocessing improves training stability by ensuring consistent input dimensions and normalized value ranges. Without preprocessing, models may fail due to dimension mismatches or struggle with convergence due to unstandardized input scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd759d",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8b102",
   "metadata": {},
   "source": [
    "### 1. Why must corrupted samples be removed before training a model?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ab73d",
   "metadata": {},
   "source": [
    "### 2. What problems can arise if images have inconsistent sizes?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078190e",
   "metadata": {},
   "source": [
    "### 3. Why is normalization important for gradient-based learning?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005cab7",
   "metadata": {},
   "source": [
    "### 4. Why should raw data never be overwritten?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236410ab",
   "metadata": {},
   "source": [
    "### 5. How does preprocessing affect model convergence and performance?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a9939",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "[Write your conclusion here including:\n",
    "- Dataset used and acquisition method\n",
    "- Key observations during data quality inspection\n",
    "- How preprocessing was structured and why\n",
    "- Importance of data cleaning before model training]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
