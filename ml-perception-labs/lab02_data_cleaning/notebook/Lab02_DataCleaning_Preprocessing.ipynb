{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Laboratory Exercise 2: Data Cleaning and Preprocessing\n",
                "\n",
                "**Name:** Lorenzo Bela, Robert Callorina, Kean Guzon\n",
                "\n",
                "**Section:** 58036\n",
                "\n",
                "**Date:** January 26 2026\n",
                "\n",
                "**Dataset:** CIFAR-10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import pickle\n",
                "from PIL import Image\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part-b",
            "metadata": {},
            "source": [
                "## Part B: Load Raw Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "path = Path('../../lab01_data_value_chain/data/raw/cifar-10-batches-py/data_batch_1')\n",
                "\n",
                "with open(path, 'rb') as f:\n",
                "    batch = pickle.load(f, encoding='bytes')\n",
                "\n",
                "all_x = batch[b'data'][:3000].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
                "all_y = np.array(batch[b'labels'][:3000])\n",
                "\n",
                "print(len(all_x))\n",
                "print(all_x[0].shape)\n",
                "print(all_x.dtype)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_raw",
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
                "for a in ax.flat:\n",
                "    idx = np.random.randint(len(all_x))\n",
                "    a.imshow(all_x[idx])\n",
                "    a.axis('off')\n",
                "plt.savefig('../outputs/figures/lab02_raw_samples.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part-c",
            "metadata": {},
            "source": [
                "## Part C: Detect Data Quality Issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "detect",
            "metadata": {},
            "outputs": [],
            "source": [
                "valid = 0\n",
                "invalid = 0\n",
                "\n",
                "for img in all_x:\n",
                "    if img.shape == (32, 32, 3):\n",
                "        valid += 1\n",
                "    else:\n",
                "        invalid += 1\n",
                "\n",
                "print(valid, invalid)\n",
                "\n",
                "with open('../outputs/logs/lab02_data_issues.txt', 'w') as f:\n",
                "    f.write(f\"Valid: {valid}\\nInvalid: {invalid}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part-d",
            "metadata": {},
            "source": [
                "## Part D: Data Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "clean",
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_x = all_x\n",
                "clean_y = all_y\n",
                "\n",
                "print(len(clean_x))\n",
                "\n",
                "pd.DataFrame({'Metric': ['Original', 'Cleaned'], 'Count': [len(all_x), len(clean_x)]}).to_csv('../outputs/tables/lab02_cleaning_summary.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part-e",
            "metadata": {},
            "source": [
                "## Part E: Image Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess",
            "metadata": {},
            "outputs": [],
            "source": [
                "proc_64 = (np.array([np.array(Image.fromarray(img).resize((64, 64))) for img in clean_x]) / 255.0).astype(np.float16)\n",
                "proc_128 = (np.array([np.array(Image.fromarray(img).resize((128, 128))) for img in clean_x]) / 255.0).astype(np.float16)\n",
                "\n",
                "print(proc_64.dtype, proc_64[0].shape, proc_64.min(), proc_64.max())\n",
                "print(proc_128.dtype, proc_128[0].shape, proc_128.min(), proc_128.max())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz_proc",
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(3, 5, figsize=(10, 6))\n",
                "idxs = np.random.choice(len(clean_x), 5)\n",
                "\n",
                "for i, idx in enumerate(idxs):\n",
                "    ax[0, i].imshow(clean_x[idx])\n",
                "    ax[1, i].imshow(proc_64[idx].astype('float32'))\n",
                "    ax[2, i].imshow(proc_128[idx].astype('float32'))\n",
                "    for a in ax[:, i]: a.axis('off')\n",
                "\n",
                "plt.savefig('../outputs/figures/lab02_processed_samples.png')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part-f",
            "metadata": {},
            "source": [
                "## Part F: Save Processed Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "out = Path('../data/processed')\n",
                "out.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "np.save(out / 'images_64x64.npy', proc_64)\n",
                "np.save(out / 'images_128x128.npy', proc_128)\n",
                "np.save(out / 'labels.npy', clean_y)\n",
                "\n",
                "print('Saved')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "meta",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('../outputs/logs/lab02_preprocessing_metadata.txt', 'w') as f:\n",
                "    f.write(f\"Date: 2026-01-26\\nOriginal: {clean_x.shape}\\nProcessed: {proc_64.shape}, {proc_128.shape}\\nNorm: 0-1\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results",
            "metadata": {},
            "source": [
                "## Results\n",
                "A. 32x32 uint8 [0-255] became 64x64/128x128 float16 [0-1].\n",
                "B. No issues found.\n",
                "C. Processed images look same but bigger.\n",
                "D. Preprocessing makes training stable."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q1",
            "metadata": {},
            "source": [
                "### 1. Why must corrupted samples be removed before training a model?\n",
                "They can cause errors and make the model learn wrong things."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q2",
            "metadata": {},
            "source": [
                "### 2. What problems can arise if images have inconsistent sizes?\n",
                "The model expects fixed input size so it will crash."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q3",
            "metadata": {},
            "source": [
                "### 3. Why is normalization important for gradient-based learning?\n",
                "It helps math work better and learning happen faster."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q4",
            "metadata": {},
            "source": [
                "### 4. Why should raw data never be overwritten?\n",
                "So we can start over if we mess up."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q5",
            "metadata": {},
            "source": [
                "### 5. How does preprocessing affect model convergence and performance?\n",
                "It makes it faster and more accurate."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusion",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "We used CIFAR-10 data. We checked it and it was clean. We resized images to 64x64 and 128x128 and divided by 255 to normalize. This is important for training models."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}